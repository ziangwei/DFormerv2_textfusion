# hsg_head.py
import torch
import torch.nn as nn
from mmcv.cnn import ConvModule
from mmseg.ops import resize
from collections.abc import Mapping
from .decode_head import BaseDecodeHead
from .ham_head import Hamburger
from ..blocks.semantic_alignment import SemanticAlignmentModule
import torch.nn.functional as F  # ğŸ”§ æ–°å¢ï¼šç”¨äº F.interpolate ä¸Šé‡‡æ ·geo_mask
import math                       # ğŸ”§ æ–°å¢ï¼šç”¨äº math.sqrt è®¡ç®—ç‰¹å¾å›¾å°ºå¯¸

class SAMStack(nn.Module):
    """æŠŠåŒä¸€å±‚çš„è‹¥å¹²ä¸ª SAM ä¸²èµ·æ¥ï¼›æ— å±‚æ—¶ç­‰ä»· Identityã€‚"""
    def __init__(self, sam_layers=None):
        super().__init__()
        self.layers = nn.ModuleList(sam_layers or [])

    def forward(self, f_chw, text_features=None,
                geo_mask=None,  # ğŸ”§ æ–°å¢
                return_attn=False):  # ğŸ”§ æ–°å¢
        if len(self.layers) == 0:
            if return_attn:
                return f_chw, None
            return f_chw

        x = f_chw.permute(0, 2, 3, 1).contiguous()
        attn_list = [] if return_attn else None

        for sam in self.layers:
            if return_attn:
                x, attn_i = sam(x, text_features, geo_mask, return_attn=True)
                attn_list.append(attn_i)
            else:
                x = sam(x, text_features, geo_mask, return_attn=False)

        out = x.permute(0, 3, 1, 2).contiguous()
        if return_attn:
            return out, attn_list
        return out

class HierarchicalSemanticGuidedHead(BaseDecodeHead):
    """
    HSGHeadï¼ˆHam æµç¨‹ + é€å±‚ SAMStackï¼‰
      [æ¯å±‚ SAMStack] â†’ ä¸Šé‡‡æ ·åˆ°åŒåˆ†è¾¨ç‡ â†’ concat â†’ 1x1 squeeze â†’ Hamburger â†’ 1x1 align â†’ cls_seg
    """

    def __init__(self,
                 in_channels,
                 in_index,
                 input_transform="multiple_select",
                 channels=512,
                 num_classes=40,
                 norm_cfg=None,
                 # æ–‡æœ¬ / ç¨€ç–æ³¨æ„åŠ›å‚æ•° â€”â€” ä¸ builder ä¿æŒä¸€è‡´çš„åç§°
                 text_dim=512,
                 sam_dec_stages=(0, 1, 2, 3),
                 backbone_num_heads=(4, 4, 8, 16),
                 sam_dec_repeats=1,
                 sam_use_topk=True,
                 sam_top_m=5,
                 **kwargs):
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¦æŠŠ sam_* ä¼ ç»™çˆ¶ç±»ï¼Œé¿å… BaseDecodeHead æŠ¥â€œunexpected keywordâ€
        super().__init__(in_channels=in_channels,
                         in_index=in_index,
                         input_transform=input_transform,
                         channels=channels,
                         num_classes=num_classes,
                         norm_cfg=norm_cfg,
                         **kwargs)

        self.ham_channels = channels
        self.text_dim = text_dim
        self.backbone_num_heads = list(backbone_num_heads)

        # === ä¸ºæ¯ä¸ªè¾“å…¥ç‰¹å¾å±‚æ„å»ºä¸€ä¸ª SAMStackï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ ===
        # builder é‡Œä¼ å‚ï¼šsam_dec_stagesã€sam_use_topkã€sam_top_mï¼Œä¸æ­¤ä¸€è‡´ã€‚:contentReference[oaicite:1]{index=1}
        global_enable = set(list(sam_dec_stages or []))
        self.dec_sam_stacks = nn.ModuleList()

        for local_i, Cin in enumerate(self.in_channels):
            # å°† head çš„ local ç´¢å¼•æ˜ å°„å› backbone çš„å…¨å±€ stage ç´¢å¼•
            global_idx = self.in_index[local_i] if isinstance(self.in_index, (list, tuple)) else local_i
            enabled = (global_idx in global_enable)

            repeat = 0
            if enabled:
                repeat = self._resolve_repeat_count(sam_dec_repeats, global_idx, local_i)
                repeat = max(int(repeat), 0)

            layers = []
            for _ in range(repeat):
                layers.append(
                    SemanticAlignmentModule(
                        query_dim=Cin,
                        text_dim=text_dim,
                        num_heads=self.backbone_num_heads[global_idx],
                        alpha_init=0.05,
                        attn_drop=0.0, proj_drop=0.0, ffn_drop=0.0,
                        # å…³é”®ï¼šä¿æŒä¸ builder çš„ sam_* å¯¹åº”ï¼ˆå†…éƒ¨æ¨¡å—ä»ç”¨ use_topk/top_m è¿™ä¸¤ä¸ªå‚æ•°åï¼‰
                        use_topk=bool(sam_use_topk),
                        top_m=int(sam_top_m),
                    )
                )
            self.dec_sam_stacks.append(SAMStack(layers))  # ç©ºåˆ™ä¸º Identity

        # === Ham çŸ­è·¯å¾„ ===
        self.squeeze = ConvModule(
            sum(self.in_channels), channels, 1,
            conv_cfg=self.conv_cfg, norm_cfg=self.norm_cfg, act_cfg=self.act_cfg
        )
        self.hamburger = Hamburger(
            ham_channels=channels, ham_kwargs=dict(),
            norm_cfg=self.norm_cfg, **kwargs
        )
        self.align = ConvModule(
            channels, self.channels, 1,
            conv_cfg=self.conv_cfg, norm_cfg=self.norm_cfg, act_cfg=self.act_cfg
        )

    def _resolve_repeat_count(self, repeat_cfg, global_idx, local_idx):
        if isinstance(repeat_cfg, Mapping):
            if global_idx in repeat_cfg: return repeat_cfg[global_idx]
            if local_idx in repeat_cfg:  return repeat_cfg[local_idx]
        elif isinstance(repeat_cfg, (list, tuple)):
            if len(repeat_cfg) == len(self.in_channels):
                return repeat_cfg[local_idx]
            if len(repeat_cfg) > 0:
                return repeat_cfg[min(local_idx, len(repeat_cfg) - 1)]
        return int(repeat_cfg)

    # ----------------------------- å‰å‘ -----------------------------
    def forward(self, inputs, text_features=None,
                geo_priors=None,  # ğŸ”§ æ–°å¢ï¼šæ¥è‡ªencoderçš„å‡ ä½•å…ˆéªŒåˆ—è¡¨
                return_attn=False):  # ğŸ”§ æ–°å¢ï¼šæ˜¯å¦è¿”å›attention maps
        feats = self._transform_inputs(inputs)

        attn_maps = [] if return_attn else None  # ğŸ”§ æ”¶é›†attention

        tgt_hw = feats[0].shape[2:]
        proc_feats = []
        for i, f in enumerate(feats):
            # ğŸ”§ æ­¥éª¤1: è·å–å¹¶å¤„ç†geo_mask
            geo_mask_i = None
            if geo_priors is not None and i < len(geo_priors):
                geo_mask_raw = geo_priors[i]  # [B, H, N_in, N_in] æˆ–å…¶ä»–å½¢çŠ¶

                # ä¸Šé‡‡æ ·åˆ°å½“å‰ç‰¹å¾å›¾å¤§å°
                B, C, H_cur, W_cur = f.shape
                N_cur = H_cur * W_cur

                # å‡è®¾geo_mask_rawæ˜¯ [B, num_heads, N_in, N_in]
                # ç®€åŒ–ï¼šå–å¯¹è§’çº¿å¹³å‡ä½œä¸ºæ¯ä¸ªåƒç´ çš„å‡ ä½•ç½®ä¿¡åº¦
                if geo_mask_raw.dim() == 4:  # [B, H, N, N]
                    # å–æ‰€æœ‰headçš„å¹³å‡
                    geo_mask_raw = geo_mask_raw.mean(dim=1)  # â†’ [B, N_in, N_in]

                if geo_mask_raw.dim() == 3:  # [B, N_in, N_in]
                    # æ–¹æ¡ˆA: å–å¯¹è§’çº¿ï¼ˆæ¯ä¸ªåƒç´ è‡ªå·±çš„å‡ ä½•å¼ºåº¦ï¼‰
                    geo_mask_i = torch.diagonal(geo_mask_raw, dim1=-2, dim2=-1)  # [B, N_in]

                    # æ’å€¼åˆ°å½“å‰åˆ†è¾¨ç‡
                    H_in = int(math.sqrt(geo_mask_i.size(1)))
                    geo_mask_i = geo_mask_i.view(B, H_in, H_in)
                    geo_mask_i = F.interpolate(
                        geo_mask_i.unsqueeze(1),  # [B, 1, H_in, H_in]
                        size=(H_cur, W_cur),
                        mode='bilinear',
                        align_corners=False
                    ).squeeze(1)  # â†’ [B, H_cur, W_cur]
                    geo_mask_i = geo_mask_i.view(B, N_cur)  # â†’ [B, N_cur]

            # ğŸ”§ æ­¥éª¤2: é€šè¿‡SAMStackï¼Œä¼ å…¥geo_mask
            if return_attn:
                f, attn_i = self.dec_sam_stacks[i](
                    f, text_features, geo_mask_i, return_attn=True
                )
                attn_maps.append(attn_i)
            else:
                f = self.dec_sam_stacks[i](f, text_features, geo_mask_i)
            f = resize(f, size=tgt_hw, mode="bilinear", align_corners=self.align_corners)
            proc_feats.append(f)

        x = torch.cat(proc_feats, dim=1)
        x = self.squeeze(x)
        x = self.hamburger(x)
        x = self.align(x)
        out = self.cls_seg(x)

        # ğŸ”§ å¯é€‰è¿”å›attention maps
        if return_attn:
            return out, attn_maps
        return out
