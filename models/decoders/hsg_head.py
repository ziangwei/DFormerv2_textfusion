# hsg_head.py
import torch
import torch.nn as nn
from mmcv.cnn import ConvModule
from mmseg.ops import resize
from collections.abc import Mapping
from .decode_head import BaseDecodeHead
from .ham_head import Hamburger
from ..blocks.semantic_alignment import SemanticAlignmentModule
import torch.nn.functional as F  # ğŸ”§ æ–°å¢ï¼šç”¨äº F.interpolate ä¸Šé‡‡æ ·geo_mask
import math                       # ğŸ”§ æ–°å¢ï¼šç”¨äº math.sqrt è®¡ç®—ç‰¹å¾å›¾å°ºå¯¸


def sobel_xy(depth: torch.Tensor):
    # depth: [B,1,H,W]
    kx = depth.new_tensor([[[[-1.,0.,1.],[-2.,0.,2.],[-1.,0.,1.]]]])
    ky = depth.new_tensor([[[[-1.,-2.,-1.],[0.,0.,0.],[1.,2.,1.]]]])
    dx = F.conv2d(depth, kx, padding=1)
    dy = F.conv2d(depth, ky, padding=1)
    return dx, dy


class SAMStack(nn.Module):
    """æŠŠåŒä¸€å±‚çš„è‹¥å¹²ä¸ª SAM ä¸²èµ·æ¥ï¼›æ— å±‚æ—¶ç­‰ä»· Identityã€‚"""
    def __init__(self, sam_layers=None):
        super().__init__()
        self.layers = nn.ModuleList(sam_layers or [])

    def forward(self, f_chw, text_features=None,
                geo_mask=None,  # ğŸ”§ æ–°å¢
                return_attn=False):  # ğŸ”§ æ–°å¢
        if len(self.layers) == 0:
            if return_attn:
                return f_chw, None
            return f_chw

        x = f_chw.permute(0, 2, 3, 1).contiguous()
        attn_list = [] if return_attn else None

        for sam in self.layers:
            if return_attn:
                x, attn_i = sam(x, text_features, geo_mask, return_attn=True)
                attn_list.append(attn_i)
            else:
                x = sam(x, text_features, geo_mask, return_attn=False)

        out = x.permute(0, 3, 1, 2).contiguous()
        if return_attn:
            return out, attn_list
        return out


class HierarchicalSemanticGuidedHead(BaseDecodeHead):
    """
    HSGHeadï¼ˆHam æµç¨‹ + é€å±‚ SAMStackï¼‰
      [æ¯å±‚ SAMStack] â†’ ä¸Šé‡‡æ ·åˆ°åŒåˆ†è¾¨ç‡ â†’ concat â†’ 1x1 squeeze â†’ Hamburger â†’ 1x1 align â†’ cls_seg
    """

    def __init__(self,
                 in_channels,
                 in_index,
                 input_transform="multiple_select",
                 channels=512,
                 num_classes=40,
                 norm_cfg=None,
                 # æ–‡æœ¬ / ç¨€ç–æ³¨æ„åŠ›å‚æ•° â€”â€” ä¸ builder ä¿æŒä¸€è‡´çš„åç§°
                 text_dim=512,
                 sam_dec_stages=(0, 1, 2, 3),
                 backbone_num_heads=(4, 4, 8, 16),
                 sam_dec_repeats=1,
                 sam_use_topk=True,
                 sam_top_m=5,
                 **kwargs):
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¦æŠŠ sam_* ä¼ ç»™çˆ¶ç±»ï¼Œé¿å… BaseDecodeHead æŠ¥â€œunexpected keywordâ€
        super().__init__(in_channels=in_channels,
                         in_index=in_index,
                         input_transform=input_transform,
                         channels=channels,
                         num_classes=num_classes,
                         norm_cfg=norm_cfg,
                         **kwargs)

        self.ham_channels = channels
        self.text_dim = text_dim
        self.backbone_num_heads = list(backbone_num_heads)

        self.dgn = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(16, 16, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(16, 2, 1)  # [:,0:1] -> gate_logit, [:,1:2] -> beta_logit
        )

        # === ä¸ºæ¯ä¸ªè¾“å…¥ç‰¹å¾å±‚æ„å»ºä¸€ä¸ª SAMStackï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ ===
        # builder é‡Œä¼ å‚ï¼šsam_dec_stagesã€sam_use_topkã€sam_top_mï¼Œä¸æ­¤ä¸€è‡´ã€‚:contentReference[oaicite:1]{index=1}
        global_enable = set(list(sam_dec_stages or []))
        self.dec_sam_stacks = nn.ModuleList()

        for local_i, Cin in enumerate(self.in_channels):
            # å°† head çš„ local ç´¢å¼•æ˜ å°„å› backbone çš„å…¨å±€ stage ç´¢å¼•
            global_idx = self.in_index[local_i] if isinstance(self.in_index, (list, tuple)) else local_i
            enabled = (global_idx in global_enable)

            repeat = 0
            if enabled:
                repeat = self._resolve_repeat_count(sam_dec_repeats, global_idx, local_i)
                repeat = max(int(repeat), 0)

            layers = []
            for _ in range(repeat):
                layers.append(
                    SemanticAlignmentModule(
                        query_dim=Cin,
                        text_dim=text_dim,
                        num_heads=self.backbone_num_heads[global_idx],
                        alpha_init=0.05,
                        attn_drop=0.0, proj_drop=0.0, ffn_drop=0.0,
                        # å…³é”®ï¼šä¿æŒä¸ builder çš„ sam_* å¯¹åº”ï¼ˆå†…éƒ¨æ¨¡å—ä»ç”¨ use_topk/top_m è¿™ä¸¤ä¸ªå‚æ•°åï¼‰
                        use_topk=bool(sam_use_topk),
                        top_m=int(sam_top_m),
                    )
                )
            self.dec_sam_stacks.append(SAMStack(layers))  # ç©ºåˆ™ä¸º Identity

        # === Ham çŸ­è·¯å¾„ ===
        self.squeeze = ConvModule(
            sum(self.in_channels), channels, 1,
            conv_cfg=self.conv_cfg, norm_cfg=self.norm_cfg, act_cfg=self.act_cfg
        )
        self.hamburger = Hamburger(
            ham_channels=channels, ham_kwargs=dict(),
            norm_cfg=self.norm_cfg, **kwargs
        )
        self.align = ConvModule(
            channels, self.channels, 1,
            conv_cfg=self.conv_cfg, norm_cfg=self.norm_cfg, act_cfg=self.act_cfg
        )

    def _resolve_repeat_count(self, repeat_cfg, global_idx, local_idx):
        if isinstance(repeat_cfg, Mapping):
            if global_idx in repeat_cfg: return repeat_cfg[global_idx]
            if local_idx in repeat_cfg:  return repeat_cfg[local_idx]
        elif isinstance(repeat_cfg, (list, tuple)):
            if len(repeat_cfg) == len(self.in_channels):
                return repeat_cfg[local_idx]
            if len(repeat_cfg) > 0:
                return repeat_cfg[min(local_idx, len(repeat_cfg) - 1)]
        return int(repeat_cfg)

    # ----------------------------- å‰å‘ -----------------------------
    def forward(self, inputs, text_features=None,
                geo_priors=None,  # ğŸ”§ æ–°å¢ï¼šæ¥è‡ªencoderçš„å‡ ä½•å…ˆéªŒåˆ—è¡¨
                return_attn=False):  # ğŸ”§ æ–°å¢ï¼šæ˜¯å¦è¿”å›attention maps
        feats = self._transform_inputs(inputs)

        attn_maps = [] if return_attn else None  # ğŸ”§ æ”¶é›†attention

        tgt_hw = feats[0].shape[2:]
        proc_feats = []
        for i, f in enumerate(feats):
            # ğŸ”§ å¤„ç† geo_mask
            geo_mask_i = None
            global_idx = self.in_index[i] if isinstance(self.in_index, (list, tuple)) else i
            if geo_priors is not None:
                try:
                    geo_raw = geo_priors[global_idx]  # ä¼˜å…ˆæŒ‰å…¨å±€ stage å–
                    if isinstance(geo_raw, torch.Tensor) and geo_raw.dim() == 4 and geo_raw.shape[1] == 1:
                        depth_i = F.interpolate(geo_raw, size=f.shape[-2:], mode='bilinear',
                                                align_corners=self.align_corners).to(f.dtype)
                        # æ·±åº¦ + æ¢¯åº¦ -> DGNï¼ˆå¯å­¦ä¹  gate/betaï¼‰
                        dx, dy = sobel_xy(depth_i)
                        d_in = torch.cat([depth_i, dx.abs(), dy.abs()], dim=1)  # [B,3,Hs,Ws]
                        d_out = self.dgn(d_in)  # [B,2,Hs,Ws]
                        gate = torch.sigmoid(d_out[:, 0:1])  # [B,1,Hs,Ws] âˆˆ (0,1)
                        beta = F.softplus(d_out[:, 1:2]) + 1.0  # [B,1,Hs,Ws] > 0
                        geo_mask_i = {"depth": depth_i, "gate": gate, "beta": beta}  # ä¼  dict
                    else:
                        geo_mask_i = geo_raw  # å…¼å®¹ [B,N]/[B,N,N]
                except Exception:
                    geo_mask_i = None
            else:
                geo_mask_i = None

            # ğŸ”§ ä¼ ç»™ SAMStack
            if return_attn:
                f, attn_i = self.dec_sam_stacks[i](f, text_features, geo_mask_i, return_attn=True)
                attn_maps.append(attn_i)
            else:
                f = self.dec_sam_stacks[i](f, text_features, geo_mask_i)

            f = resize(f, size=tgt_hw, mode="bilinear", align_corners=self.align_corners)
            proc_feats.append(f)

        x = torch.cat(proc_feats, dim=1)
        x = self.squeeze(x)
        x = self.hamburger(x)
        x = self.align(x)
        out = self.cls_seg(x)

        # ğŸ”§ å¯é€‰è¿”å›attention maps
        if return_attn:
            return out, attn_maps
        return out
